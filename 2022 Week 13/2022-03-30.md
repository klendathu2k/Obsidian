Note: Teleworking from home today b/c heat is off in office... Plan to work from home tomorrow and commute to the lab on Friday.

- sPHENIX poduction system
	- Coming to a few design decisions...
		- What I *don't* want to do is implement a CWL/YAML workflow for every single pass of every single sPHENIX data production.  This would be cumbersome.
		- The solution I am coming to is to create a single CWL/YAML workflow that is used to wrap a *user provided shell script*.  The CWL/YAML file will
			- Setup the environment (search paths, software build level, etc...)
			- And create environment variables which the user script can pickup:
				- N events (or event range), N jobs, N files per job, unique ID for the job, input file list, unique integer (aka index, rngseed, ...) for each job, etc...
		- Currently have a CWL workflow which can run the pass1 simulation.  
		- User provides a run_xxx.sh script, and modifies the yaml file to point to the script, specify the number of events, etc...
		- This is essentially the approach taken by SUMS.  SUMS maps all of the job's parameters (N files, file list, N events, etc...) onto environment variables which the user can pickup / apply in their shell script.
		- This gets us to the point where we can execute one single linear workflow.
		- To build up dependent behaviors, will need to implement as CWL workflows that invoke the generic workflows with user scripts to provide the inputs.  The more complicated workflows ... may or may not be generalized in the way that the linear workflows are.

[[STAR S&C Meeting 03-30-2022]]
- Victor's github journey...
- [PR #335](https://github.com/star-bnl/star-sw/pull/335/files)
	- Contains geometry updates



