- SHREK
	- Define a task as a process that takes zero or more inputs and transforms them into one or more outputs
	- A task requires a definition file, which specifies the input data sets and the output data sets, plus all parameters and codes necessary to perform the transformation
	- Tasks may be executed within other tasks, to form more complicated workflows...
	- In PanDA this is described to the system by specifying a CWL document, describing the relationships between tasks...
	- If we drop the full definition into a single file, we can and express the organization of tasks using the unix directory structure, then we can automate the process of creating the CWL.  Users only need to define each task (with a yaml file) and organize their tasks into an appropriate filesystem hierarchy.
	- Thnk System V unix initialization.
	- On the user side, workflow is defined by organizing files into directories.
	- On the system side, workflow is built by traversing the directories, establishing a CWL file for each directory, converting the user provided yaml file into a shell script which is submitted via pchain.
	- Yaml is a very open format, permitting essentially freeform input.  There should be correctness checking at some point.
	- Yaml --> python dict --> python classes --> workflow spec for PanDA
		- python classes should be used to enforce schema upon user...

## SHREK yaml schema

### User defined variables and arrays...

Variables can be defined for consumption by the user code(s) which will be run on the remote worker nodes.  SHREK will build a job script utilizing the *bash* scripting language.  Anything declared as a "name: value" pair (or values) will be exported as environment variables w/in the script.  There is no output made to the workflow.

| yaml | job script | workflow |
| -----| -------------| --------|
| name: value | export name=value | ---- |
| name: (values) | export name=(values) | ---- |

### User defined mappings?
It may be possible to support user-defined [associative arrays](https://stackoverflow.com/questions/688849/associative-arrays-in-shell-scripts)

### Datasets
inputDataSets:
- name: 
     comment: a user comment field (required)
	 altname: a user defined variable name (optional)
     nFilesPerJob: number of files per job (optional, defaults to 1)
     match: pattern matching applied to filenams (optional)
     nSkip: number of files to skip (optional)
	 nFiles: limited number of files (optional, may only be specified on first dataset)
	 filelist: (optional)
	   - path::/path/to/files/on/system/*
	 
  Input data sets are declared as a list of named data sets.  Minimally a comment is required (describing the DS being used).  Optionally the number of files per job can be set, as well as the criteria for matching files and the number of files to skip in the dataset.
  
  Files passed to the user script will be listed in an array carrying the name of the dataset.  In the event that the name is too long, contains special characters, etc...  an alternate name can be specified... 
  
  filelist: an optional parameter specifying a list of files (either specifically listing all files, or selecting a set of files by wildxard).  More than one 
  
  The first dataset is taken as the primary dataset for the