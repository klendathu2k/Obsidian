SHREK
---
Likely to need an in-memory representation of the graph of job descriptions we are building.  It would also be nice to have some level of error checking during the build process, so we can quickly identify and debug errors in the workflow.  So ideally we would want to have a python based graph library.

https://wiki.python.org/moin/PythonGraphLibraries

[python-graph](https://github.com/Shoobx/python-graph)
- Stable, reasonably active, have to build code using make? documentation lacking?  (built with make?)

[networkx](https://networkx.org/)
- Stable, reasonably active, extensive documentation in easy reach...

[graph-tool](https://graph-tool.skewed.de/)
- ... 

Basically narrows down to the last two... will do a quick evaluation.  The API looks a little bit simlper / easier w/ networkx.  6 vs 1/2 dozen.  Go with networkx (pip install-able).
... and, it is already installed (may be used under the hood by PanDA or some other previously installed tool).

---

Next need to settle on another implementation detail.

To describe the workflow graph, we have several options...

- Organize code into directories, using the names of the directories to denote the direction of the graph...

```
+ 001_SIMULATION/params.yaml
+ 002_PILEUP/params.yaml
+ 003_RECONSTRUCTION/params.yaml
|
+--+ 001_CALORIMETER/params.yaml
+--+ 001_TRACKING/params.yaml
|  |
|  +--+ 001_CALIBRATION/params.yaml
|  +--+ 002_TRACK_FINDING/params.yaml
|  +--+ 003_TRACK_FITTING/params.yaml
|
+ 004_MERGE/params.yaml                   
```

- Organize code into directories, with parameters holding same names as directories...

```
+ 001_SIMULATION.yaml
+ 001_SIMULATION/
+ 002_PILEUP.yaml
+ 002_PILEUP/
+ 003_RECONSTRUCTION.yaml
+ 003_RECONSTRUCTION/
|
+--+ 001_CALORIMETER.yaml
+--+ 001_CALORIMETER/
+--+ 001_TRACKING.yaml
+--+ 001_TRACKING/
|  |
|  +--+ 001_CALIBRATION.yaml
|  +--+ 001_CALIBRATION/
|  +--+ 002_TRACK_FINDING.yaml
|  +--+ 002_TRACK_FINDING/
|  +--+ 003_TRACK_FITTING.yaml
|  +--+ 003_TRACK_FITTING/
|
+ 004_MERGE.yaml                   
+ 004_MERGE/
```

Laying the code out into directories like this has the advantage that users can quickly see the structure... but one could also flatten the design...

```
100_SIMULATION.yaml
100_SIMULATION/
200_PILEUP.yaml
200_PILEUP/
300_RECONSTRUCTION.yaml
300_RECONSTRUCTION/
310_CALORIMETER.yaml
310_CALORIMETER/
310_TRACKING.yaml
310_TRACKING/

311_CALIBRATION.yaml
311_CALIBRATION/
312_TRACK_FINDING.yaml
312_TRACK_FINDING/
313_TRACK_FITTING.yaml
313_TRACK_FITTING/

400_MERGE.yaml                   
400_MERGE/
```

Each of these schemes cuts *against* the single-source goal.  In that we are encoding the graph in the naming convention of the yaml files / source directories...

The purpose of this organization is user-support / visualization of the structure.  But we should be able to visualize using graphviz (or other such tool).  

We should be able to build up the job graph (needed to create the CWL file for PanDA submission) by linking outputs of jobs to inputs of other jobs...

May as well make a test of it.  Mock up the first two passes of the HF charm production...
- runPass1.yaml
- runPass2.yaml

Output of pass1 goes into input of pass2...  at the moment, this is the only thing to test.  Can we build the graph from which the CWL will be built?  (The parameters will come later...)  

First, validation of the pass1 and pass2 yaml files fail b/c pass2 requires a secondary dataset.  At the moment, the schema only permits a single input DS.  So I should either extend the schema to allow for multiple InputDS's...  or I should add an additional schema for secondary DS's.

- Looking at prun --helpGroup INPUT...  there is facility to declare multiple input data sets.  So... 
- InputDataSet should become a list of IDS.
- SecondaryDataSet should be a list of secondary datasets...

Having difficulty defining a schema to allow a list of maps...
Looking at this further... should probably add this to a TODO list.   Support running over multple data sets at a later date... b/c the PanDA prun cmd allows a different level of flexability than my simple(ish) schema permits.

... but no, I need multiple data sets for input b/c the reconstruction workflow will eventually have a merge step.  And although PanDA seems to support a merge script option... I don't know that I plan to rely on that, as it seems redundant with being able to execute arbitrary payloads.
... have it working in the standalone, but not as part of a job definition...
By all that is good and sweet in Odin's beard!

