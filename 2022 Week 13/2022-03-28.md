sPHENIX Production / SHREK

No *wonder* I am having problems with PanDA.  I'm reading the [wrong documentation](https://panda.readthedocs.io/en/latest/index.html)!

- Interacted w/ Tadashi by email... job submission issue seemed to be due to lack of output DS being defined...  
	- sometimes you just need to see the log file...
	- this revealed a panda/pchain/idds bug (not really sure where the bug was...) that resulted in the job being silently killed (no error reported, but no jobs / error messages show up in the monioring...)  
	- as usual... I seem to find the unexpected/untested corner cases
- Plan for the day is to keep pushing to setup pass 2.  Need to review notes from last week to figure out where I got detoured onto the "OMG why is this not working" track...
- Reviewing... 
	- able to submit single-pass run_hfprod.sh jobs 

>prun --exec "run_hfprod.sh mdc2.7 1 Charm TestSimulation /sphenix/user/sphnxpro/shrek/pass1/out 1234567890 %RNDM:00001" --outDS user.jwebb2.uuidgen --noBuild --vo wlcg --site BNL_OSG_SPHENIX --prodSourceLabel test --workingGroup ${PANDA_AUTH_VO} --maxAttempt 1 --followLinks --cpuTimePerEvent=1200 --maxWalltime 24 --memory 2048 --nJobs=5`

>$ ls /sphenix/user/sphnxpro/shrek/pass1/out -l
total 44288
-rw-r--r-- 1 sphnxpro sphenix 9213282 Mar 24 11:31 G4Test_Charm-1000000001-00001.root
-rw-r--r-- 1 sphnxpro sphenix 7298995 Mar 24 11:36 G4Test_Charm-1000000001-00002.root
-rw-r--r-- 1 sphnxpro sphenix 8402272 Mar 24 11:37 G4Test_Charm-1000000001-00003.root
-rw-r--r-- 1 sphnxpro sphenix 6745177 Mar 24 11:35 G4Test_Charm-1000000001-00004.root
-rw-r--r-- 1 sphnxpro sphenix 8682391 Mar 24 11:38 G4Test_Charm-1000000001-00005.root
-rw-r--r-- 1 sphnxpro sphenix 2735998 Mar 24 09:49 G4Test_Charm-1234567890-00002.root
-rw-r--r-- 1 sphnxpro sphenix 1052504 Mar 24 09:54 G4Test_Charm-1234567890-00004.root

- Reviewing... but had difficulties submitting CWL via pchain b/c of the aforementioned corner case I found.  
	- Standing in the corner... brings back so many memories.
	- Okay... one thing is bothering me... ![[Pasted image 20220328111605.png|200]]
	- ... why did I remove the DS on the command line?  b/c the CWL workflow was failing b/c the bottom chain could not find the output from the top chain.  And this was a cut-and-paste pchain example... so may indicate an issue with rucio integration?  But I would expect Rucio to only come into play at the end when copy back is initiated?  TBD.

-  Reviewing...
	-  Open question from last week was how to parameterize the jobs... specifically, in the CWL we invoke *prun*... we don't invoke the run_hfprod_passN.sh scripts.  
		-  IF we were not restricted to executing prun, THEN I could bind each arguement of the run_hfprod_passN.sh script to a CWL entity, and configure their values in the YAML file.
		-  With PRUN, the run_hfprod_passN.sh script is invoked *as the exec arguement* to prun.  This is a single arguement which can be bound to a single CWL entity.
			-  But we need to pass in multiple arguements.  Which means that either the YAML file needs to be generated dynamically to serve the job, OR ... I need to find some magic to allow me to bind CWL entities together this way.
			-  Another option... CWL allows us to create a file dynamically.... so.... I can define all of the input parameters in the YAML file... dynamically create a script which defines all of the job parameters that I need to propagate, and then execute that script from within the run_hfprod_passN script.
				-  Only issue is that certain PanDA variables still need to be passed as command line arguements... e.g. %IN
				-  

- It is time to fork [sPHENIX MDC2](https://github.com/klendathu2k/MDC2)
	- and will begin to build up a set of PanDA job submission scripts/ macros/ areas for the HF production campaigns.
	- once able to perform the workflow, can begin to generalize / systematize the creation of the workflows, make connects with inputs, outputs, logs, archival, etc...

- So... possible solution here.  Use CWL facilities to create a job configuration script to be executed as part of the job driver script.  (Or... create the full job script?)


- Have created a simple CWL file which creates an `init-env.sh` script, that accepts input from the YAML input file.  This script can then be used as part of the run_hfprod_passN.sh macro, so that job parameters (nevents, etc...) can be exposed.
	- Now the problem is how to run this as part of a panda workflow...
	- Starting with the simple example... not much fun...
	- Starting with the background mixing sample... better result...
		- It at least submitted the workflow to the PanDA.  Now to the monitor...
	- Expanding on the simple file... would like to be able to specify initial set of macros to copy into local run area, so that we can swap out and make the actual set of macros a configurable part of the job.
		- [Staging files](https://www.commonwl.org/user_guide/15-staging/)



- There are some software packages which can be used to simpliify the creation of CWL files... I am thinking that scriptcwl in particular could be used as part of a pythonic interface to setup and run jobs.  Once a job is up and running, an entry could be added into a crontab to monitor for job completion, and then report back once all output files have been returned... report back on production status... report on any error conditions detected...  blah blah.   Although scriptcwl doesn't look as if it is being acitively developed... and may be something so simple that rolling our own would be a preferable option?  TBD.
	- [Janis Transpiler](https://github.com/PMCC-BioinformaticsCore/janis)
	- [scriptcwl python driver](https://github.com/NLeSC/scriptcwl)


Notes...
	- The cleanup continues...
	![|200](https://lh6.googleusercontent.com/mnNhJJ5hfZD1Uu-HoU0eLFpRQLLHOSqXuSF5lb5e4IA8Z9E0iu7RZHVC5ZQ8jsXJvOJIr1gJL8vp48aVaE48onL3vjqOBR52aUeHbdtC2TVM8duHd2ZzD8bOxc9qsGfq4FB1jxWZ)