Note: Teleworking from home today b/c heat is off in office... Plan to work from home tomorrow and commute to the lab on Friday.

- sPHENIX poduction system
	- Coming to a few design decisions...
		- What I *don't* want to do is implement a CWL/YAML workflow for every single pass of every single sPHENIX data production.  This would be cumbersome.
		- The solution I am coming to is to create a single CWL/YAML workflow that is used to wrap a *user provided shell script*.  The CWL/YAML file will
			- Setup the environment (search paths, software build level, etc...)
			- And create environment variables which the user script can pickup:
				- N events (or event range), N jobs, N files per job, unique ID for the job, input file list, unique integer (aka index, rngseed, ...) for each job, etc...
		- Currently have a CWL workflow which can run the pass1 simulation.  
		- User provides a run_xxx.sh script, and modifies the yaml file to point to the script, specify the number of events, etc...
		- This is essentially the approach taken by SUMS.  SUMS maps all of the job's parameters (N files, file list, N events, etc...) onto environment variables which the user can pickup / apply in their shell script.
		- This gets us to the point where we can execute one single linear workflow.
		- To build up more con