## Thursday 04/21

STAR infrastructure mattermost is blowing up right now.

SHREK
---



Few notes:

1) Bug in current code.  When a job has no input, the parser state is not cleared from the previous yaml file.  Possibly from other parameters as well.  This results in input field (and other fields) recieving data from previous file.
	--> Solution: new instance of a handler for each yaml file parsed
2) b/c we use jobs as nodes and output/input as edges... there are no edges shown for pre-staged inputs.
3) Building the graph is a bit of overkill for the type of workflows sPHENIX is likely to employ... CWL already handles building the network based on a fairly flat description of steps.  Still, by building the network I can at least order the CWL document in a way that is easier to comprehend.  
4) Data set names... 
	- I think I have a small confusion between the name of a dataset, and the tokens used in the CWL document
	- There seems to be a convention in the panda workflow examples for the output data sets...
		- stepname/outDS refers to the [outDS] specified in the output block of the named step... 
		- I don't recall why the [outDS] is wrapped in an array expression... presuming that CWL allows multiple output data sets...
		- per PanDA pchain documentation: `The "out" section specifies the task output with an arbitrary string surrendered by brackets. Note that it is always a single string even if the task produces multiple outputs.`
		- So the out field of the step is arbitrary... and a single string, not really an array expression.  (But using the arry syntax).  
		- So... when building the workflow documents for submission, we should follow this convention.  out: [outDS] in the step, and stepname/outDS to reference the given output dataset.
		- 
	
