## Wednesday {{date}}

- [ ] 0830AM [NPPS Coffee](https://bnl.zoomgov.com/j/16157150845?pwd=NXNqTi9ZWEFBKzYwRXQ5U3NXU1dBZz09)
- [ ] 1000AM [sPHENIX Distributed Computing](https://bnl.zoomgov.com/j/16157150845?pwd=NXNqTi9ZWEFBKzYwRXQ5U3NXU1dBZz09)
- [ ] 1200PM [STAR S&C Meeting](https://lbnl.zoom.us/j/97026562983?pwd=VGVXbzhYUUhheEJ2cFMyVVdVRXowZz09)


Yesterday's run...
[successful](https://panda-doma.cern.ch/task/66719/)
[unsuccessful](https://panda-doma.cern.ch/task/66722/)

... and I don't see the difference.  So perhaps I need to run with maxAttempt > 1

It may be that the third job is starting before the requirements are satisfied?  May be an issue on the server side?  (Again?  Really?)

#!!@# ^&  IT WAS SERVER SIDE.  Things are running now.  

Okay... still not seeing my code unpack the tarballs that the input data files are wrapped into...

Test on local machine... I can unpack tarballs... assuming, of course, that they are in the working directory.  So 

--


And... wait a tick... am I missing something fundamental?  Expectation was that we iterate over files in data sets.  Pileup job should be launched for each file in the input set.   But I am seeing %IN --> $IN1 containing multiple input files.  Por qui?

Try specifying 1 file per pileup job.

--


