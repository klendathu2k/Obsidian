## Thursday {{date}}

### sPHENIX
 
500 signal jobs + 2k BG jobs (reusable) 
https://panda-doma.cern.ch/tasks/?jeditaskid=132498|132497|132499|132518|132496

Ran to completion.  Output files for each of the reconstruction jobs look to be reasonable in size.  Can we scale further?

Try doubling up on the input list... (does PanDA check for duplication here
... no.  PanDA does not allow dataset duplication here (it drops the duplicate definitions.)

Okay.  Moving on.  Let's setup the new workflow that Chris informed me about.

First, tag the MDC2 directory @ shrek-mdc2.8

Next, update remote.  Pull.  And branch from main shrek-ana.315.

```
> pass4_jobC needs input from an averaging over 30  
> minutes worth of data from pass4_jobA outputs but we don't do this  
> right now.

  
So... let me make sure that I understand this... because that "averaging" could become  
a "bottleneck"... and we may need some error handling in the production system to  
make sure we don't get stuck waiting on the average.  
  
...  
  
In the real production, the workflow for N input "files" would look like...  
  
(N x job0) --> (N x jobA) --> (average over N outputs) --> (N x jobC )  
  
where the (average over N outputs) can be done over the first 30 minutes  
worth of data.
```

### STAR

Pythia8 update... again, Dmitri is pushing to separate out the interface from the external packages.  Yes, in an ideal world it is the correct thing to do.  No, it is not worth the effort here.   At least not for the moment. 



