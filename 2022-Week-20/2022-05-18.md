## Wednesday {{date}}

- [ ] 0830AM [NPPS Coffee](https://bnl.zoomgov.com/j/16157150845?pwd=NXNqTi9ZWEFBKzYwRXQ5U3NXU1dBZz09)
- [ ] 1000AM [sPHENIX Distributed Computing](https://bnl.zoomgov.com/j/16157150845?pwd=NXNqTi9ZWEFBKzYwRXQ5U3NXU1dBZz09)
- [ ] 1200PM [STAR S&C Meeting](https://lbnl.zoom.us/j/97026562983?pwd=VGVXbzhYUUhheEJ2cFMyVVdVRXowZz09)


Yesterday's run...
[successful](https://panda-doma.cern.ch/task/66719/)
[unsuccessful](https://panda-doma.cern.ch/task/66722/)

... and I don't see the difference.  So perhaps I need to run with maxAttempt > 1

It may be that the third job is starting before the requirements are satisfied?  May be an issue on the server side?  (Again?  Really?)

#!!@# ^&  IT WAS SERVER SIDE.  Things are running now.  

Okay... still not seeing my code unpack the tarballs that the input data files are wrapped into...

Test on local machine... I can unpack tarballs... assuming, of course, that they are in the working directory.  So 

So...  pipe the output of the shell script into a log file on the sphnxpro directory...

```
...
IN1
tar (child): user.jwebb2.66747._000001.G4XYZ.root.tgz: Cannot open: No such file or directory
tar (child): Error is not recoverable: exiting now
tar: Child returned status 2
...
```

tar does not see the file.  ???  this is completely odd... ls tells me these files are there.  But tar throws no such file?  dafuq?  

--


And... wait a tick... am I missing something fundamental?  Expectation was that we iterate over files in data sets.  Pileup job should be launched for each file in the input set.   But I am seeing %IN --> $IN1 containing multiple input files.  Por qui?

Try specifying 1 file per pileup job.

*--> nFilesPerJob (on primary input) is not exported to the workflow file <--*

--


