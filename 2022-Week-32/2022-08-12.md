## Friday {{date}}

- [ ] 1000AM [NPPS Weekly Meeting](https://docs.google.com/document/d/1YfTyXPeXNQU4XUB28bvHJolgyBIJ2bfrd0u9Gd3WD70/edit)[zoom](https://bnl.zoomgov.com/j/16157150845?pwd=NXNqTi9ZWEFBKzYwRXQ5U3NXU1dBZz09)
- [ ] TODO: Weekly Summary

### sPHENIX 
--------------------------------


Setup / run using the old workflow
https://panda-doma.cern.ch/tasks/?taskname=user.jwebb2.sP22r-hfcharm-pileup-test5-oldwf_*

https://panda-doma.cern.ch/tasks/?jeditaskid=132550|132548|132549|132547

... appears to be working fine...

(except... the pileup output files all )


Setup new workflow...

pass4 --> 3 jobs... pass4_job0, jobA and jobC.  There is no apparent difference between the call arguements of job0, jobA and jobC.  Some difference with the original.  Can use this as a starting point.

Submitted...

https://panda-doma.cern.ch/tasks/?taskname=user.jwebb2.sP22r-hfcharm-pileup-test5-newwf_*

... a few false starts... b/c (1) the macros packages were not correctly defined for the new tracking codes, and (2) the mergeOutputs definition did not set the right input from the StageC tracking.

Looks like this will take through tomorrow to see whether the new tracking codes are setup correctly... (dum dum... should have limited this to 10 files...)

Thinking ahead... we need to get some fault tolerance in place.  Results downstream of the distortion corrections need to depend on the DC.  And this will be a merge operation, which could potentially stall the processing...

Seeing some oddball timing in some of these pileup jobs... 250 run quick, then the rest are ~h slow.  How / why?
