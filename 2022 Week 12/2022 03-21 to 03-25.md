StEvent Maintenence
- Able to convert the (ulta-legacy) LaTeX documentation into markdown, suitable for becoming the README.md in the StRoot/StEvent/ directory... this will make the document available / browsable on github.  
	- images (.eps files) do not show, but otherwise readable / understandable
- Used the "pandoc" application on my home linux system
- We can convert any (all) .tex documents sitting in the doc/ areas in the repo
- Will need to fix my repository's main branch (got sloppy and committed something directly to it...)
- The StEvent documentation is woefully out of date... predates Barrel and Endcap calorimeters (circa 2003 then)

sPHENIX Workflows / Production System 

- Working title: sPHENIX Handy Remote Execution Coordinator  (SHREK)
![[Pasted image 20220323093933.png|200]]


- By dumping log files straight to disk (/sphenix/users/sphnxpro/out.txt) was able to start testing the pass 1 production macros.
- Needed to reshape scripts / macros slightly... b/c we need to encapsulate the complete job into a single script, passing in the software release level, to initialize the sPHENIX environment
- Verified that jobs run, environment is correct, they obtain independent random seeds (sequence number), and produce output.
- Copying back (for now) direct to disk.  Will sort out rucio access after we can run through the pass 2 workflow (pileup against pregenerated backrgound events).

- Email exchange with Tadashi clarified the usage of CWL in PanDA.  
	- The CWL files are executed on the PanDA server  (which is documented).   
	- The CWL files should *only* execute *prun* tasks in the workflow (which is not documented)
	- The reason for the latter restriction (I believe) is because the CWL file is executed in a plain vanilla CWL runner, that doesn't know anything about the remote resources being used, or how to parallelize jobs across multiple systems.  
	- This solves some problems
		- I no longer need to figure out how to convince CWL to stage support files... pchain does this automagically
	- This also raises some problems...
		- How to build up the "exec" parameter for the prun task from other CWL parameters.  (The CWL language documentation es no beuno.)
	
	- What is the utility of runnig the CWL workflow on a remote PanDA server?   The CWL runner could be executing locally on an rcas node (well.. sphenix node).  
		- The local job setup, the execution on the batch system, and the job bookeeping task, could then be merged into and managed by a single workflow document.
		- Sending this off to a server for submission does allow the jobs to be run on some remote service (e.g. @ NERSC, or amazon, or...)  but all should be prepped, and all should be documented at the end.
		- Tradeoff to consider (and we can always steer the creation of the job, the submission via pchain, and the archival with a locally executed workflow).
	
	

Miscellaneous
- Discussed w/ Chris... sPHENIX still has an issue with their MC truth tagging, in that they keep ***everything*** in memory.  Full particle production / decay / interaction history.  (*bleh*).   Memory explosion.  Maybe worth taking a stab at this problem later.


		

[[2022-03-21]]
[[2022-03-22]]
[[2022-03-23]]
[[2022-03-24]]
[[2022-03-25]]
[[sPhenix Handy Remote Execution Coordinator]]
